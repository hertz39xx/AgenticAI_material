## Week 11: Adaptive to New knowledge with Model Tuning
### 0. Assignment 2 Summary [TA] 
### 1. Group Presentation of the Reading assignments
- Team 8: Presentation [15-20 mintues]
- Team 4: Challenging by asking questions [5 mintues]
### 2. Open source LLMs and Model fine-tuning [ [Slide](https://docs.google.com/presentation/d/11-rgEMHoe1Gh925yTCbMU9qJJhBX9_c3v9SqNulBmdY/edit?usp=sharing) ]
- Open-source LLMs: Mixture of Experts, Latent Dirichlet Allocation, EM (Expectef Maimization) 
- LLM API Providers
- Install local Models

- Fine-Tuning via PEFT, LoRA

- Training with Unsloth ![alt text](<Devoteam_rebirth_blog_prompt-engineering-vs-RAG-2 (1).png>)

### 3. Hands-on Practice
- [Ollama](https://ollama.com/)
- [Unsloth](https://unsloth.ai/)
### 4. Reading Assignments:
- [Continual Learning for Large Language Models: A Survey](https://arxiv.org/pdf/2402.01364)
- Mixture of Experts: Knowledge Distillation
  - https://chatgpt.com/share/687c5921-5fbc-8007-853d-a95d6dd5213e